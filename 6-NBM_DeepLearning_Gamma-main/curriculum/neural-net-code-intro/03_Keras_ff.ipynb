{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:13:38.277389Z",
     "start_time": "2019-05-20T13:13:35.024031Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for tabular data\n",
    "\n",
    "## Summary of Deep Learning Methods\n",
    "\n",
    "Today we will cover the most fundamental application of deep learning:\n",
    "- **Feedforward** Neural Networks maximize flexibility. They are appropriate in cases where we shouldn't make assumptions about relationships between our input features. They do especially well on tabular data, like the dataframes we've seen so far. \n",
    "\n",
    "Other important techniques of deep learning:\n",
    "- **Embeddings** are a technique for learning efficient relationships between categories.\n",
    "- **Convolutional** Neural Networks are use to model spatial relationships. They are particularly useful in image and audio tasks but have many more applications.\n",
    "- **Recurrent** Neural Networks are used to model sequences of data, like sentences or time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of this notebook\n",
    "\n",
    "- Demonstrate best practices for deep learning on tabular data.\n",
    "- Discuss common techniques for applying and improving deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will load in a the Ames Housing Data, split into train and test sets, and build some models. (www.amstat.org/publications/jse/v19n3/decock.pdf). \n",
    "\n",
    "In the following cells I will perform the basic data cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:16:04.231954Z",
     "start_time": "2019-05-20T13:16:03.593264Z"
    }
   },
   "outputs": [],
   "source": [
    "# I'm going to load in the data and take care of the data cleaning here.\n",
    "\n",
    "ames_df=pd.read_csv(\"http://www.amstat.org/publications/jse/v19n3/decock/AmesHousing.txt\", sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "y_orig = ames_df[\"SalePrice\"]\n",
    "x_orig = ames_df.drop(columns=\"SalePrice\")\n",
    "\n",
    "ames_dummies = pd.get_dummies(x_orig, dummy_na=True)\n",
    "ames_dummies = ames_dummies.fillna(0)\n",
    "\n",
    "x_orig = x_orig.select_dtypes('number')\n",
    "x_orig = x_orig.fillna(0)\n",
    "\n",
    "\n",
    "x_orig = x_orig.merge(ames_dummies, left_index=True, right_index=True)\n",
    "x_orig = x_orig.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>**Important**: we previously said that neural nets require minimal pre-processing. One of those processing steps is scaling our input data. It's recommended that you scale (normalize) all input data when using deep learning/neural nets.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:16:08.607048Z",
     "start_time": "2019-05-20T13:16:08.515624Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, model_selection\n",
    "\n",
    "\n",
    "x, x_test, y, y_test = model_selection.train_test_split(x_orig,y_orig)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "x_test = scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a test set on which we can compare our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:16:13.625790Z",
     "start_time": "2019-05-20T13:16:13.617873Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "def benchmark(model):\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    print(f\"mae: {metrics.mean_absolute_error(y_test, y_pred):,.2f}\")\n",
    "    print(f\"mse: {metrics.mean_squared_error(y_test, y_pred):,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "\n",
    "When we care about performance, random forests are a great off-the-shelf model for tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:16:22.114317Z",
     "start_time": "2019-05-20T13:16:14.941935Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "\n",
    "rf = ensemble.RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "rf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:16:23.794041Z",
     "start_time": "2019-05-20T13:16:23.748790Z"
    }
   },
   "outputs": [],
   "source": [
    "benchmark(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this RF as our baseline. Let's train a plain neural net for to do the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple neural net\n",
    "\n",
    "\n",
    "Let's build a two-layer network just as we did in the Neural Net Theory notebook. The difference here is that we will not use an activation function on the output.\n",
    "\n",
    "\n",
    "<mark>For regression problems, you will typically use a linear (no) activation function on your final layer.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:16:28.468015Z",
     "start_time": "2019-05-20T13:16:28.272703Z"
    }
   },
   "outputs": [],
   "source": [
    "ff_model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=x.shape[1:]),\n",
    "    keras.layers.Dense(units=5, activation=\"relu\"),\n",
    "    keras.layers.Dense(units=1),\n",
    "])\n",
    "ff_model.compile(\"sgd\", loss=\"mean_absolute_error\", metrics=[\"mean_squared_error\"])\n",
    "ff_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:16:39.748086Z",
     "start_time": "2019-05-20T13:16:36.682371Z"
    }
   },
   "outputs": [],
   "source": [
    "ff_model.fit(x, y, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:09:44.694350Z",
     "start_time": "2019-05-09T21:09:44.634584Z"
    }
   },
   "outputs": [],
   "source": [
    "benchmark(ff_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.dropbox.com/s/46x057it18kuhh3/2019-03-01_09-16-00.png?raw=1)\n",
    "Here's a look at some new things used in the above cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f59f374922c498bf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Check for understanding\n",
    "\n",
    "**Question**: the output layer of the model above has 6 trainable parameters. Where does that number come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-dfc330ac454af5e7",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Each dense layer includes the linear transformation and possibly an activation function\n",
    "\n",
    "$$f_a(\\mathbf{w}^\\top\\mathbf{x} + \\mathbf{b})$$\n",
    "\n",
    "We know that the input to the second layer has a dimensionality of $[5 \\times n]$ where $n$ is the number of examples in our minibatch. We also know that the output of the second layer is $[1 \\times n]$. These both come from setting `units=5` in the first layer and `units=1` in the second.\n",
    "\n",
    "what do $\\mathbf{w}$ and $\\mathbf{b}$ need to look like in order to scale the units from 5 to 1? $\\mathbf{w}$ will need to be $[5 \\times 1]$ and $\\mathbf{b}$ will need to be $[1 \\times 1]$. That leaves 5 trainable parameters in $\\mathbf{w}$ and just one in $\\mathbf{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful tools üõ†\n",
    "\n",
    "Okay, so that's our basic, vanilla neural network. It's a good starting point. Let's look at a few additions that might help us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "```python\n",
    "keras.layers.Dropout(0.05)\n",
    "```\n",
    "\n",
    "Dropout is a layer that randomly selects a percentage of the weights and sets them to zero. The percentage above is 5%. Dropout only applies in the training phase of the model and is turned of when we use `model.predict(...)`.\n",
    "\n",
    "Dropout is part of a broad view of regularization that has become more common with Deep Learning techniques. Before you might have defined regularization as something like \"adding weights to your loss function\". From now on, we'll use regularization to refer to any technique that penalizes training performance in order to improve test performance.\n",
    "\n",
    "\n",
    "> <mark>\"Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.\"</mark> [Deep Learning](https://www.deeplearningbook.org/) by Goodfellow, Bengio and Courville"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight regularization\n",
    "\n",
    "```python\n",
    "keras.layers.Dense(units=300, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "```\n",
    "\n",
    "You can still use regularization on the weights of your neural network with the `kernel_regularizer=` parameter. This works just like in ridge/lasso regression that you learned about before. Regularization can also be applied to the bias (less common) and the activation for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced optimizers\n",
    "\n",
    "```python\n",
    "test_model.compile(\n",
    "    keras.optimizers.adam(lr=0.001), loss=\"mean_absolute_error\",\n",
    "    metrics=[\"mean_squared_error\"])\n",
    "```\n",
    "\n",
    "We previously used the default optimizer (\"sgd\" for stochastic gradient descent) but we have a lot of options. You can use any of the optimizers from the keras package and set their parameters yourself. \n",
    "\n",
    "[See the list of keras optimizers.](https://keras.io/optimizers/)\n",
    "\n",
    "**Setting the learning rate**: If you set `lr` too high, the model won't learn because the . Too low and it won't learn because it isn't moving far enough. The default values are often good places to start. <mark>Tip: it often works best to set the learning rate to the highest rate where the model still improves with each epoch.</mark>\n",
    "\n",
    "**Good default**: a good default is to start with `keras.optimizers.adam`. This is a strong-performing advanced optimizer that incorporates momentum and adapts the learning rate for each parameter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Callbacks are helpful functions that are run after an epoch or batch. There are callbacks in keras and they are very, very helpful. \n",
    "\n",
    "[See the list of keras callbacks.](https://keras.io/callbacks/)\n",
    "\n",
    "```python\n",
    "test_model.fit(\n",
    "    x, y, epochs=100, batch_size=100, validation_split=.25, verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=8, verbose=1, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=.5, patience=3, verbose=1),\n",
    "    ])\n",
    "```\n",
    "\n",
    "### `ReduceLROnPlateau`\n",
    "\n",
    "![](https://www.dropbox.com/s/bplh2nbnyaus682/2019-03-01_10-26-06.png?raw=1)\n",
    "\n",
    "`ReduceLROnPlateau` reduces the learning rate after the model stops improving. You set how aggressively this happens with the `patience` parameter.\n",
    " \n",
    "Why would you want to do this? The learning rate balances how quickly the model descends the loss gradient vs how precisely it does so. A high learning rate moves more quickly but cannot precisely find minima. A low learning rate can carefully fall into a minima but it does so very slowly. In practice, modelers have found that using learning rate schedules is a helpful way to allow your model to quickly find minima at the beginning of training and then later more precisely target those minima to improve performance. In the figure here (reproduced from [Clevert, Unterthiner & Hochreiter](https://arxiv.org/abs/1511.07289)) you can see a common pattern where learning plateaus at one learning rate and then accelerates again as soon as the learning rate is lowered.\n",
    "\n",
    "`ReduceLROnPlateau` is convenient because it allows you to perform this automatically without designing a learning rate schedule by hand. \n",
    "\n",
    "### `EarlyStopping`\n",
    "\n",
    "How many epochs should you tell your model to use? There's no good answer to this question but thankfully we can use `EarlyStopping` to tell the model to stop whenever performance no longer improving. <mark>Set `patience` here to be higher than `ReduceLROnPlateau` if you're using both.</mark> With `restore_best_weights=True` the model will restore the weights of the best epoch after it's finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:09:45.045660Z",
     "start_time": "2019-05-09T21:09:44.700244Z"
    }
   },
   "outputs": [],
   "source": [
    "test_model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=x.shape[1:]),\n",
    "    keras.layers.Dense(units=300, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "    keras.layers.Dropout(0.01),\n",
    "    keras.layers.Dense(units=200, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "    keras.layers.Dense(units=100, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "    keras.layers.Dense(units=50, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "    keras.layers.Dense(units=1),\n",
    "])\n",
    "test_model.compile(\n",
    "    keras.optimizers.Adam(lr=0.001), loss=\"mean_absolute_error\",\n",
    "    metrics=[\"mean_squared_error\"])\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:10:07.239607Z",
     "start_time": "2019-05-09T21:09:45.049598Z"
    }
   },
   "outputs": [],
   "source": [
    "test_model.fit(\n",
    "    x, y, epochs=100, validation_split=.25, verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=8, verbose=1, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=.5, patience=3, verbose=1),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:10:07.465186Z",
     "start_time": "2019-05-09T21:10:07.257356Z"
    }
   },
   "outputs": [],
   "source": [
    "benchmark(test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-450d7395be5ba3eb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# In-class exercise (you code)\n",
    "\n",
    "The model above is overfitting. How can you tell?\n",
    "\n",
    "\n",
    "In the cell below, build a similar model but address the problem of overfitting. See if this improves performance on the test set.\n",
    "\n",
    "You should name your model `student_model`. Feel free to start with the boilerplate below\n",
    "\n",
    "```python\n",
    "student_model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=x.shape[1:]),\n",
    "    ...\n",
    "])\n",
    "student_model.compile(\n",
    "    keras.optimizers.adam(lr=0.001), loss=\"mean_absolute_error\",\n",
    "    metrics=[\"mean_squared_error\"])\n",
    "student_model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-556284169006d15f",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "<small>[Note from Sophie Searcy]</small>  \n",
    "I often find that L2 regularization is effective for tabular regression tasks. The trick is to make sure you are scaling your L2 multiplier correctly. You want to set the multiplier so that your model pays attention to both your regression loss and regularization loss. \n",
    "\n",
    "Also note that when we use regularization, we can no longer assume that the model loss (which is your total loss for your model) and regression loss (the loss for your regression task) are the same. In the case below, the loss reported after each epoch is now `mean_absolute_error+regularization`. To get a measure of the regression loss, I add `mean_absolute_error` back into the metrics. This way I know what each component of the loss is after each epoch. \n",
    "\n",
    "The last epoch for the solution below should log something like `loss: 20266.6511 - mean_absolute_error: 8115.6820`. With that information I know that my regularization loss is `20266.6511 - 8115.6820=12150.9690`. Even though the regularization loss is most of the total loss at the end, the model is still overfitting! That's okay, because we still are getting validation and test performance that's beating our random forest above üòâ.\n",
    "\n",
    "Also note that we are using both dropout and L2 regularization. I find that the combination tends to work better that using one of them alone but your mileage may vary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:10:08.151337Z",
     "start_time": "2019-05-09T21:10:07.470501Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-50f90991a8757e6c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    " \n",
    "\n",
    "L2 = 50\n",
    "DROP = 0.05\n",
    "student_model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=x.shape[1:]),\n",
    "    keras.layers.Dense(units=140, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(L2)),\n",
    "    keras.layers.Dropout(DROP),\n",
    "    keras.layers.Dense(units=120, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(L2)),\n",
    "    keras.layers.Dropout(DROP),\n",
    "    keras.layers.Dense(units=100, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(L2)),\n",
    "    keras.layers.Dropout(DROP),\n",
    "    keras.layers.Dense(units=80, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(L2)),\n",
    "    keras.layers.Dropout(DROP),\n",
    "    keras.layers.Dense(units=60, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(L2)),\n",
    "    keras.layers.Dropout(.5*DROP),\n",
    "    keras.layers.Dense(units=40, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(L2)),\n",
    "    keras.layers.Dropout(.5*DROP),\n",
    "    keras.layers.Dense(units=20, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(L2)),\n",
    "#     keras.layers.Dropout(DROP),\n",
    "    keras.layers.Dense(units=1),\n",
    "])\n",
    "student_model.compile(\n",
    "    keras.optimizers.Adam(lr=0.01), loss=\"mean_absolute_error\",\n",
    "    metrics=[\"mean_absolute_error\", \"mean_squared_error\"])\n",
    "student_model.summary()\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:10:37.593774Z",
     "start_time": "2019-05-09T21:10:08.154454Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "student_model.fit(\n",
    "    x, y, epochs=500, validation_split=.25, verbose=1, callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            patience=16,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            factor=.5,\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "        ),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:10:38.346928Z",
     "start_time": "2019-05-09T21:10:37.603312Z"
    }
   },
   "outputs": [],
   "source": [
    "benchmark(student_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9253ae6b098e7b42",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Question: addressing overfitting in deep learning\n",
    "\n",
    "Given the tools you've seen so far, if you have a deep learning model that's overfitting, how can you address this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-05c97ffa1cb3e051",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "# Question: addressing overfitting in deep learning\n",
    "\n",
    "- Reduce parameters by reducing the number of units in each layer or reducing the number of layers.\n",
    "- Add dropout layers (or increase the dropout rate)\n",
    "- Add regularization (or increase the strength)\n",
    "- There are others that we haven't discussed yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aside: How do you choose the size of each layer?\n",
    "\n",
    "The model in the above solution has the following layer sizes.\n",
    "\n",
    "- (input) 387\n",
    "- 120\n",
    "- 100\n",
    "- 80\n",
    "- 60\n",
    "- 40\n",
    "- 20\n",
    "- 1 (output layer)\n",
    "\n",
    "How should you choose these sizes when designing your own model?\n",
    "\n",
    "First, your input layer size is fixed by your input. This is also true of your output layer size which is fixed based on the task. Here's two examples of common tasks:\n",
    "- Regression on a single variable: `units=1`\n",
    "- Categorization with multiple categories: `units=len(unique(y))`\n",
    "\n",
    "That gives us our start and end point. Here are two rules of thumb:\n",
    "\n",
    "**Linear increase**: increase the size of each layer by a fixed amount. (like the above)\n",
    "\n",
    "**Geometric increase**: increase the size of each layer by a fixed multiplier. (like the below)\n",
    "\n",
    "- (input) 387\n",
    "- 64\n",
    "- 32\n",
    "- 16\n",
    "- 8\n",
    "- 4\n",
    "- 2\n",
    "- 1 (output layer)\n",
    "\n",
    "*How do you choose between the two?* I would consider them a generic hyperparameter that can be tweaked. If you're trying to squeeze every ounce of performance out of a model, compare the cross-validated performance of each. Otherwise, just pick the one you're most comfortable with and stick with it.\n",
    "\n",
    "*How do you choose the size of increase?* This should be considered a hyperparameter that affects model complexity. A bigger step size and you'll have more parameters and a more complex model that is more likely to overfit. You can use this to tune your model.  \n",
    "<small>(I like to limit the step size so that the first layer is smaller than the input size, mostly for aesthetic reasons.)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concluding\n",
    "\n",
    "Okay, that's a lot of information and a lot of (hyper)parameters to worry about! Technically, because we can always add more layers, there is an infinite number of hyperparameters in every model. That's kind of daunting! üôÖ‚Äç Here's some final advice on how to deal with this problem and a little demo of how to use sklearn's `GridSearchCV`.\n",
    "\n",
    "## Advice\n",
    "- <mark>Start with an existing model that works</mark>. If there's a working model in a paper, blog-post, Metis lesson notebook, by all means *start* there. Then, as you're making this model your own, change it one step at a time and make sure each step doesn't break things.\n",
    "- Look for ways to <mark>reduce the things you need to worry about</mark>. Examples: I almost always leave `batch_size` at the default because it (typically) doesn't meaningfully affect training. I also use `EarlyStopping` in every model so I don't have to set the number of epochs.\n",
    "- <mark>Every change that you make should be for a reason</mark>. It's easy to get lost in the infinite tiny decisions in every deep learning model. Being strategic about the changes that you're making will help you navigate this problem. Examples:\n",
    "    - I want give my model room to be more complex *because I think it's underfit*. I'll add layers and/or increase the size of existing layers. I might also reduce my regularization settings if I already am using that.\n",
    "    - My model performance is not reliably improving between epochs. I want to test out different optimizers and learning rates *because I think there's an optimization problem*.\n",
    "    - I want to use regularization on my model *because I think it's overfit*. I'll add dropout or regulariation or increase the strength of those layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## `GridSearchCV` demo\n",
    "\n",
    "Even taking the advice above, we still have a bunch of decisions to make. Here's a demo using sklearn's grid search to tackle a bunch of hyperparameters. We just set up a function to build a model based on the hyperparameters and then let it go to town üíÅ‚Äç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:12:11.848766Z",
     "start_time": "2019-05-09T21:12:11.795281Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_model(base=20, add=0, noise=.0, drop=0, depth=10, batchnorm=False,\n",
    "               act_reg=0.01, kern_reg=0.01, lr=0.01):\n",
    "\n",
    "    layers = [keras.layers.InputLayer(input_shape=x.shape[1:])]\n",
    "\n",
    "    if noise > 0:\n",
    "        layers.append(keras.layers.GaussianNoise(noise))\n",
    "\n",
    "    for mult in range(depth, 0, -1):\n",
    "        layers.append(\n",
    "            keras.layers.Dense(\n",
    "                units=mult * base + add, activation=\"relu\",\n",
    "                kernel_regularizer=keras.regularizers.l2(kern_reg),\n",
    "                activity_regularizer=keras.regularizers.l2(act_reg)),\n",
    "        )\n",
    "        if batchnorm and (mult % batchnorm == 0):\n",
    "            layers.append(keras.layers.BatchNormalization())\n",
    "        if drop > 0:\n",
    "            if mult == 0:\n",
    "                pass\n",
    "            if mult == 1:\n",
    "                pass\n",
    "            if mult == 2:\n",
    "                layers.append(keras.layers.Dropout(.5*drop))\n",
    "            else:\n",
    "                layers.append(keras.layers.Dropout(drop))\n",
    "\n",
    "    layers.append(keras.layers.Dense(1))\n",
    "\n",
    "    model = keras.Sequential(layers)\n",
    "\n",
    "    model.compile(keras.optimizers.Adam(lr=lr), loss=\"mae\", metrics=[\"mae\", \"mse\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:12:29.598108Z",
     "start_time": "2019-05-09T21:12:14.307206Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_model = make_model(base=10, add=10, depth=5, drop=0.01, batchnorm=False, act_reg=0, kern_reg=50)\n",
    "test_model.summary()\n",
    "test_model.fit(\n",
    "    x, y, epochs=200, validation_split=.25, verbose=1, callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            patience=8,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            factor=.2,\n",
    "            patience=3,\n",
    "            verbose=1,\n",
    "        ),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:11:02.642798Z",
     "start_time": "2019-05-09T21:11:02.306966Z"
    }
   },
   "outputs": [],
   "source": [
    "benchmark(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:13:13.840706Z",
     "start_time": "2019-05-09T21:13:13.801208Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.wrappers import scikit_learn as k_sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "keras_model = k_sklearn.KerasRegressor(make_model)\n",
    "\n",
    "validator = model_selection.GridSearchCV(\n",
    "    keras_model, param_grid={\n",
    "        'base': [10, 20], \n",
    "        'noise': [0],\n",
    "        'depth': [5, 10],\n",
    "        'drop': [0, 0.01],\n",
    "        'act_reg':[0], \n",
    "        'kern_reg':[0,10],\n",
    "        'batchnorm': [False],\n",
    "    }, scoring='neg_mean_absolute_error', n_jobs=-1, cv=3, verbose=2)\n",
    "\n",
    "# Uncomment when you're ready to run. This one will take a while\n",
    "\n",
    "# validator.fit(\n",
    "#     x, y, epochs=200, validation_split=.25, verbose=0, callbacks=[\n",
    "#         keras.callbacks.EarlyStopping(\n",
    "#             patience=8,\n",
    "#             verbose=0,\n",
    "#         ),\n",
    "#         keras.callbacks.ReduceLROnPlateau(\n",
    "#             factor=.2,\n",
    "#             patience=3,\n",
    "#             verbose=0,\n",
    "#         ),\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:11:42.657298Z",
     "start_time": "2019-05-09T21:09:16.095Z"
    }
   },
   "outputs": [],
   "source": [
    "# y_pred = validator.predict(x_test)\n",
    "# print(f\"mae: {metrics.mean_absolute_error(y_test, y_pred):,.2f}\")\n",
    "# print(f\"mse: {metrics.mean_squared_error(y_test, y_pred):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T21:11:42.659701Z",
     "start_time": "2019-05-09T21:09:16.099Z"
    }
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(validator.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
