{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/6/65/RothwellMaryShelley.jpg) \n",
    "**The task**: Today we're going to implement a skipgrams model to learn word vectors. We'll use the text of Mary Wollstonecraft Shelley's novel *Frankenstein; or, The Modern Prometheus* \n",
    "\n",
    "\n",
    "I've given you an outline of the code you'll need to get this running. You'll need to fill in the missing parts that I've marked.\n",
    "\n",
    "Your deliverables:\n",
    "- Word vectors for the 5,000 most frequent words in Frankenstein\n",
    "- A function that takes a target word and uses the word vectors to find the most similar words to the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:56:12.076979Z",
     "start_time": "2019-12-10T22:56:08.960317Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2ea8f4841c2552ab",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "# standard code block #\n",
    "#######################\n",
    "\n",
    "# see https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "%pylab inline\n",
    "\n",
    "# sets backend to render higher res images\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "\n",
    "#######################\n",
    "#       imports       #\n",
    "#######################\n",
    "# import pandas as pd\n",
    "import seaborn as sns\n",
    "# import sklearn\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:56:14.664916Z",
     "start_time": "2019-12-10T22:56:13.376419Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2cfe64ebf9168046",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# First, let's grab the text of Frankenstein\n",
    "# (thankfully, it's in the public domain!)\n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'http://umich.edu/~umfandsf/other/ebooks/frank10.txt'\n",
    "r = requests.get(url)\n",
    "\n",
    "shelley_text = r.text\n",
    "\n",
    "print(shelley_text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:56:38.884750Z",
     "start_time": "2019-12-10T22:56:38.864108Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b88536ef33dabe5c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Next, we'll pre-process the text by adding periods where\n",
    "# weant sentence breaks to occur.\n",
    "import re\n",
    "\n",
    "# replace 2 or more new lines with a period and one new line\n",
    "shelley_text = re.sub(r\"\\n{2,}\", \".  \", shelley_text)\n",
    "\n",
    "# replace single new lines with a space\n",
    "shelley_text = re.sub(r\"\\n\", \" \", shelley_text)\n",
    "print(shelley_text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:56:44.212499Z",
     "start_time": "2019-12-10T22:56:43.261383Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5befa2fb6481438c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Now we're going to use NLTK's sentence tokenizer to split the text into sentences. Be careful! The sentence tokenizer is sensitive to preprocessing steps like lowercasing text.\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(shelley_text)\n",
    "\n",
    "sentences = sentences[4:]\n",
    "\n",
    "print(f\"We found {len(sentences)} sentences!\", end=\"\\n\\n\")\n",
    "\n",
    "for sent in sentences[:3]:\n",
    "    print(sent[:200], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:07:15.184452Z",
     "start_time": "2019-12-10T23:07:15.018467Z"
    }
   },
   "outputs": [],
   "source": [
    "# the next step uses CountVectorizer to build a sentence vocabulary\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvect = CountVectorizer(max_features=2500)\n",
    "\n",
    "bow = countvect.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "The next step, you're going to write. We need to build the observations we'll use to train the skipgram model.\n",
    "\n",
    "The skipgram task asks a model to predict the center word given the words in the target's context. For instance, if we set `window_size=2`, then the first skipgram observation is\n",
    "\n",
    "$$\n",
    "\\underbrace{\\text{You},  \\text{will}}_\\text{context},  \\overbrace{\\text{rejoice}}^\\text{target},  \\underbrace{\\text{to},  \\text{hear}}_\\text{context}\n",
    "$$\n",
    "\n",
    "And the second observation is\n",
    "\n",
    "$$\n",
    "\\underbrace{\\text{will},  \\text{rejoice}}_\\text{context},  \\overbrace{\\text{to}}^\\text{target},  \\underbrace{\\text{hear},  \\text{that}}_\\text{context}\n",
    "$$\n",
    "\n",
    "Using `window_size=2`, construct two lists:\n",
    "- a list of context observations\n",
    "- a list of targets\n",
    "\n",
    "\n",
    "Here's what the first five look like\n",
    "\n",
    "```python\n",
    ">>>context[:5]\n",
    "['you will rejoice hear',\n",
    " 'will rejoice to that',\n",
    " 'rejoice to hear no',\n",
    " 'to hear that disaster',\n",
    " 'hear that no has']\n",
    ">>>target[:5]\n",
    "['to', 'hear', 'that', 'no', 'disaster']\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "And the first sentence boundary\n",
    "\n",
    "```python\n",
    ">>>context[15:20]\n",
    "['which you regarded with',\n",
    " 'you have with such',\n",
    " 'have regarded such evil',\n",
    " 'regarded with evil forebodings',\n",
    " 'arrived here and my']\n",
    ">>>target[15:20]\n",
    "['have', 'regarded', 'with', 'such', 'yesterday']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:07:15.686579Z",
     "start_time": "2019-12-10T23:07:15.673386Z"
    }
   },
   "outputs": [],
   "source": [
    "# The analyzer reproduces the preprocessing and tokenization\n",
    "# used in CountVectorizer. You will probably need this to split\n",
    "# sentences up in a way that our CountVectorizer understands\n",
    "\n",
    "analyzer = countvect.build_analyzer()\n",
    "\n",
    "# an example of what this looks like\n",
    "analyzer(\n",
    "    \"You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:07:16.001391Z",
     "start_time": "2019-12-10T23:07:15.991674Z"
    }
   },
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "chunk_size = window_size * 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:07:16.513418Z",
     "start_time": "2019-12-10T23:07:16.314325Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-088c117eb66614f2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "context = []\n",
    "target = []\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:07:16.709066Z",
     "start_time": "2019-12-10T23:07:16.700264Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8e98819569d4e733",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# provided tests\n",
    "assert context[:5] == [\n",
    "    'you will to hear',\n",
    "    'will rejoice hear that',\n",
    "    'rejoice to that no',\n",
    "    'to hear no disaster',\n",
    "    'hear that disaster has',\n",
    "]\n",
    "assert target[:5] == ['rejoice', 'to', 'hear', 'that', 'no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:07:16.989501Z",
     "start_time": "2019-12-10T23:07:16.977468Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9b43a88c527cf4d0",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert context[15:20] == [\n",
    "    'which you regarded with',\n",
    "    'you have with such',\n",
    "    'have regarded such evil',\n",
    "    'regarded with evil forebodings',\n",
    "    'arrived here and my',\n",
    "]\n",
    "assert target[15:20] == ['have', 'regarded', 'with', 'such', 'yesterday']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff06c395d86f479f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Ultimately, we want our inputs and outputs to be in a form that our models can understand. So we'll use a bag-of-words style encoding where each row will count the number of times a word appears in the context.\n",
    "\n",
    "Thankfully, the CountVectorizer we just trained can do this for us!\n",
    "\n",
    "The same model also has the index of each word stored in `countvect.vocabulary_`. We'll use -1 for any word not in the vocabulary (remember that we're only looking at the 1,000 most frequent words).\n",
    "\n",
    "Now our `X` a bag-of-words representation of the context for each target we found in the text. Examine the nonzero elements in the rows to verify:\n",
    "\n",
    "```python\n",
    ">>>context[0]\n",
    "'you will to hear'\n",
    ">>>nonzero(X[0,:])\n",
    "(array([0, 0, 0, 0], dtype=int32), array([412, 882, 967, 994], dtype=int32))\n",
    ">>>countvect.get_feature_names()[412]\n",
    "'hear'\n",
    ">>>countvect.get_feature_names()[882]\n",
    "'to'\n",
    ">>>countvect.get_feature_names()[967]\n",
    "'will'\n",
    ">>>countvect.get_feature_names()[994]\n",
    "'you'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:07:18.190591Z",
     "start_time": "2019-12-10T23:07:17.755134Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b7faf7df18ae4549",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X = countvect.transform(context)\n",
    "# use -1 to represent\n",
    "y = array([countvect.vocabulary_.get(t, -1) for t in target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ecb137d479ab8c07",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "Now that we have our `X` and `y`, let's train a neural network! \n",
    "\n",
    "Import scikit-learn's `MLPClassifier` and train it on our data. For skipgrams to work, you should only use one hidden layer (hint: the size of the layer determines the size of your word vectors) and you should use a linear activation. It's also recommended that you limit the number of iterations so that training terminates in a reasonable time. You can set `verbose=True` to get a picture of how the model is training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:07:18.640841Z",
     "start_time": "2019-12-10T23:07:18.630694Z"
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:09:03.179242Z",
     "start_time": "2019-12-10T23:07:19.408074Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f83cbbba0a56c57",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "The last step is to use these vectors! Write a function `get_similar(query, n=10)` that returns the `n` most similar words to `query` based on your vectors. Some helpful hints:\n",
    "\n",
    "- `mlp.coefs_` let's us access the weights of the MLPClassifier \n",
    "- `feature_names = countvect.get_feature_names()` will give us a list of all the words in the vocabulary so that we can use `feature_names[i]` to get the ith word.\n",
    "- Your word vectors are likely very high dimensional. Which distance metric works best for high dimensional data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:23:03.811543Z",
     "start_time": "2019-12-10T23:23:03.773604Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f6c2b7e3ad748dd9",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "# Try it out!\n",
    "get_similar(\"creature\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:38:16.665004Z",
     "start_time": "2019-12-10T23:38:16.651006Z"
    }
   },
   "outputs": [],
   "source": [
    "#victor's wife and adopted sister\n",
    "get_similar(\"elizabeth\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:24:00.242799Z",
     "start_time": "2019-12-10T23:24:00.214291Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "get_similar(\"justine\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra\n",
    "\n",
    "It's not part of the exercise, but below is a demo of how we can use these word vectors to look at character relationships and similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:40:43.409001Z",
     "start_time": "2019-12-10T23:40:43.374129Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "nn.fit(word_vecs)\n",
    "\n",
    "word_indices = set()\n",
    "\n",
    "key_words = [\n",
    "    \"monster\",\n",
    "    \"safie\",\n",
    "    \"agatha\",\n",
    "    \"creature\",\n",
    "    \"elizabeth\",\n",
    "    \"victor\",\n",
    "    \"frankenstein\",\n",
    "    \"henry\",\n",
    "    \"justine\",\n",
    "    \"william\",\n",
    "    \"felix\",\n",
    "]\n",
    "\n",
    "for key in key_words:\n",
    "    word_index = countvect.vocabulary_[key]\n",
    "    dist, index = nn.kneighbors(word_vecs[[word_index], :], n_neighbors=10)\n",
    "    word_indices.add(word_index)\n",
    "    word_indices.update(list(index.flat))\n",
    "\n",
    "word_indices = list(word_indices)\n",
    "\n",
    "target_vecs = word_vecs[word_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:42:01.701214Z",
     "start_time": "2019-12-10T23:41:59.551444Z"
    }
   },
   "outputs": [],
   "source": [
    "figsize(15, 10)\n",
    "\n",
    "vecs_emb = manifold.TSNE(metric=\"cosine\",\n",
    "                         n_iter=3000).fit_transform(target_vecs)\n",
    "\n",
    "sns.scatterplot(x=vecs_emb[:, 0], y=vecs_emb[:, 1], marker='.')\n",
    "for i, row in enumerate(vecs_emb):\n",
    "    word_i = word_indices[i]\n",
    "    word = feature_names[word_i]\n",
    "    if word in key_words:\n",
    "        annotate(word, row, color=\"red\", zorder=10,\n",
    "                 bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"k\", alpha=.5))\n",
    "    else:\n",
    "        annotate(word, row, zorder=-1, bbox=dict(boxstyle=\"round\", fc=\"w\",\n",
    "                                                 ec=\"k\", alpha=.5))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
