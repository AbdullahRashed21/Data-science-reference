{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: \n",
    "\n",
    "We will use the weight-based function to solve a small problem with an SVM. The function has been incorporated into a widget that appears below.\n",
    "\n",
    "You will need to find values for $\\mathbf{w}$ and $b$ that give the best classifier.\n",
    "\n",
    "$$\\underbrace{\\mathbf{w}^\\top\\mathbf{x}_\\text{test} + b}_\\text{Weight-based function}\n",
    "    = b + \\sum^{m}_{i=1}\\alpha_i\\mathbf{x}_{\\text{test}}^\\top \\mathbf{x}_{\\text{train}}[i,:]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution:\n",
    "\n",
    "One solution is $\\mathbf{w} = [1,1]$, $b=-2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise:\n",
    "\n",
    "Now, we'll solve the same problem with an SVM using the equivalent example-based function. The function has been incorporated into a widget that appears below.\n",
    "\n",
    "You will need to find values for $\\mathbf{a}$ and $b$ that give the best classifier.\n",
    "\n",
    "$$\\mathbf{w}^\\top\\mathbf{x}_\\text{test} + b\n",
    "    = \\underbrace{\n",
    "        b + \\sum^{m}_{i=1}\\alpha_i\\mathbf{x}_{\\text{test}}^\\top \\mathbf{x}_{\\text{train}}[i,:]\n",
    "       }_\\text{Example-based function}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solution\n",
    "\n",
    "One solution that works here is $\\mathbf{a} = [0,0,0,1,-1,0]$, $b = -4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discussion\n",
    "\n",
    "- Go back to the example-based widget.\n",
    "- Compare two solutions that both get 100% accuracy. Do you see any advantages to either solution?  \n",
    "    - Solution 1: $a=[1, -1, 1, 1, -1, -1]$\n",
    "    - Solution 2: $a=[0, 0, 0, 1, -1, 0]$  \n",
    "    \n",
    "    \n",
    "- If we keep $b=0$ our model is perfectly accurate. We could actually use several different values for $b$ and get the same score. How should decide between them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discussion Answers\n",
    "\n",
    "- Recall from the regression lesson that we can use Lasso to do feature selection. Lasso regression pushes the coefficient for many features to zero. This is a clue that our model might be better off if we omitted those zero features.\n",
    "- SVMs use a similar process for examples. Instead of saving all of the training examples, SVMs only need to save the ones with a non-zero $a_i$.\n",
    "    - The saved examples are called <u>**support vectors**</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://www.dropbox.com/s/lsbse60wi31lhl0/2018-12-06_11-59-43.png?raw=1)\n",
    "- Let's think back to discussions of cross validation. While the widget above is displaying training accuracy, what we really care about is how accurate a model will be on new data. Errors on new data are often called **generalization error**.\n",
    "- SVMs chose a $b$ that puts the greatest distance between examples and the decision boundary.\n",
    "    - This strategy is what is called a <u>**maximum margin**</u> classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion:\n",
    "\n",
    "Adjust the $C$ parameter. Describe how the model and its decision boundary are changing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion Answers\n",
    "\n",
    "- Higher $C$: tends to care more about increasing accuracy on the training set.\n",
    "- Lower $C$: tends to care more about reducing generalization error.\n",
    "- Very low $C$: results in all points being support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discussion\n",
    "\n",
    "Test out the different kernels and datasets. Get a feel for what each parameter does. \n",
    "\n",
    "Describe in your own words\n",
    "- What sorts of functions do the poly and rbf kernels tend to learn?\n",
    "- What does the `gamma` parameter do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion Answers\n",
    "\n",
    "- What sorts of functions do the poly and rbf kernels tend to learn?\n",
    "\n",
    "\n",
    "The **poly kernel** tends to learn continuous, curved boundaries.The function implemented by the poly kernel is: \n",
    "    \n",
    "$$K(\\mathbf{x},\\mathbf{x}') = \\left(\\gamma\\mathbf{x}^\\top\\mathbf{x} + r \\right)^d $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What sorts of functions do the poly and rbf kernels tend to learn?\n",
    "\n",
    "\n",
    "The **RBF kernel** tends to learn continuous, curved boundaries. The function implemented by the RBF kernel is: \n",
    "    \n",
    "$$K(\\mathbf{x},\\mathbf{x}') = e^{-\\gamma\\|\\mathbf{x}-\\mathbf{x}' \\|^2} $$\n",
    "\n",
    "\n",
    "What's happening under the hood is that the RBF kernel is comparing test points to each of the support vectors. When the test point is close to a support vector, that support vector's coefficient is weighted highly. When a test point is farther away, the coefficient is weighted lower. \n",
    "\n",
    "Note: you might recognize that this function is similar to the PDF of the Normal distribution, \n",
    "    \n",
    "$$f(x \\mid \\mu, \\sigma^2) =\\frac{1}{ \\sqrt{2\\pi\\sigma^2}} e^{\\frac{ (x-x')^2}{2\\sigma^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- What does the `gamma` parameter do?\n",
    "\n",
    "\n",
    "Note that in the RBF kernel, $\\gamma \\sim \\frac{1}{\\sigma^2}$, so if we think of the RBF as a Normal distribution, increasing `gamma` is analogous to decreasing the standard deviation. \n",
    "\n",
    "In more qualitative terms:\n",
    "- decreasing `gamma` leads to smoother boundaries\n",
    "- increasing `gamma` leads to boundaries that follow the training data more tightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "Confirm that SVMs with the with SGD and kernel approximation are similar to kernel SVMs. \n",
    "\n",
    "- What differences do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solution\n",
    "\n",
    "- What differences do you notice?\n",
    "    - There are no support vectors. `SGDClassifier` learns a weight matrix according to the weight-based linear function: $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + B$.\n",
    "    - The hyperparameter `C` doesn't affect the SGD SVM. This is because `SGDClassifier` uses a separate parameter `alpha` which, depending on the kernel, is roughly $\\alpha \\sim \\frac{1}{c}$.\n",
    "    - Accuracy tends to increase as the number of components increases. However, be careful because the time to train also increases with the number of components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
