{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "# standard code block #\n",
    "#######################\n",
    "\n",
    "# see https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "%pylab inline\n",
    "\n",
    "# sets backend to render higher res images\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "\n",
    "#######################\n",
    "#       imports       #\n",
    "#######################\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sns.palplot(sns.color_palette(\"husl\", 4))\n",
    "\n",
    "pal = dict(enumerate(sns.color_palette(\"husl\", 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What do we mean by \"linear\"?\n",
    "\n",
    "- Linear operations include\n",
    "    - matrix multiplication, $\\mathbf{w}^\\top\\mathbf{x}$\n",
    "    - addition, $\\mathbf{x}+b$\n",
    "- A **Linear function** is any function with the form $f(\\mathbf{x}) = \\mathbf{w}^\\top\\mathbf{x}+b$. [[more info](https://en.wikipedia.org/wiki/Linear_function)]\n",
    "\n",
    "<small> In this notebook, the weight matrix $\\mathbf{w}$ will be a $1 \\times n_f$ dimensional matrix where $n_f$ is the number of features. The bias $b$ will be a scalar. If $n_f=1$ then this definition reduces to scalar math  $f(x) = wx+b$.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Any combination of linear operations can always be reduced to a single $f(\\mathbf{x}) = \\mathbf{w}^\\top\\mathbf{x}+b$ for some $\\mathbf{w}$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The problem: \n",
    "    - What if we want a *non*linear function?\n",
    "    - But we only have linear pieces?\n",
    "- This case is a fundamental limitation of models built on linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Models\n",
    "\n",
    "- A model that is restricted to linear operations cannot learn non-linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which models are linear?\n",
    "    - Linear regression (of course!)\n",
    "    - Logistic regression\n",
    "    - Linear Support Vector Machines (of course!)\n",
    "    - (very) old Neural Networks\n",
    "    - Gaussian NaÃ¯ve Bayes$^*$\n",
    "        - Not technically linear but restricted to only learning a single mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To see this in action, we'll make a few blobs of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "x, y = datasets.make_blobs(\n",
    "    n_samples=400, n_features=2, centers=[[0, 0], [0, 10], [10, 0], [10, 10]], random_state=0)\n",
    "\n",
    "blob_df = pd.DataFrame({\"x0\": x[:,0], \"x1\": x[:,1], \"y\":y})\n",
    "\n",
    "sns.scatterplot(x=\"x0\", y=\"x1\", hue=\"y\", data=blob_df, alpha=.3, edgecolor=None, palette=pal)\n",
    "title(\"Four random gaussian clusters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then we'll subset the blobs into a classification problem that can be captured by linear functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "blob_df[\"y_sep\"] = (blob_df.y > 0).astype(\"int\")\n",
    "\n",
    "sns.scatterplot(x=\"x0\", y=\"x1\", hue=\"y_sep\", data=blob_df, alpha=.5, edgecolor=None, palette=pal)\n",
    "title(\"Seperable Clusters\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And and a classification problem that cannot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "blob_df[\"y_non\"] = blob_df.y.isin([1,2]).astype(\"int\")\n",
    "\n",
    "sns.scatterplot(x=\"x0\", y=\"x1\", hue=\"y_non\", data=blob_df, alpha=.5, edgecolor=None, palette=pal)\n",
    "title(\"Non-Separable clusters\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this out interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def plot_decision_boundary(pred_func, x, y, ax=None, points=1e3):\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = subplots()\n",
    "    \n",
    "    y_pred = pred_func(x)\n",
    "    score = metrics.accuracy_score(y_pred.flatten(), y.flatten())\n",
    "\n",
    "    sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=y, alpha=.5, edgecolor=None, palette=pal, ax=ax)\n",
    "\n",
    "    side_pts = int(sqrt(points))\n",
    "\n",
    "    x0_min, x0_max = ax.get_xlim()\n",
    "    x1_min, x1_max = ax.get_ylim()\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x0_min, x0_max, num=side_pts),\n",
    "        np.linspace(x1_min, x1_max, num=side_pts))\n",
    "\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.text(\n",
    "        (x0_min + x0_max) / 2,\n",
    "        (x1_min + x1_max) / 2,\n",
    "        f\"acc: {score:.1%}\",\n",
    "        bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"black\"))\n",
    "    \n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.2, colors=list(pal.values()), zorder=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise prep\n",
    "\n",
    "We'll classify points based on $f(\\mathbf{x})$:\n",
    "\n",
    "$$\\mathbf{w} = \n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "w_1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$ f(\\mathbf{x}) = \\mathbf{w}^\\top\\mathbf{x} + b > 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def linear_func(w0, w1, b):\n",
    "    w = array([[w0, w1]])\n",
    "    pred_func = lambda x: (w.dot(x.T) + b > 0).astype(int)\n",
    "\n",
    "    plot_decision_boundary(\n",
    "        pred_func,\n",
    "        x=blob_df[[\"x0\", \"x1\"]].values,\n",
    "        y=blob_df[\"y_sep\"].values,\n",
    "        points=1e5)\n",
    "    plt.title(f\"Accuracy for f(x) = [{w0}, {w1}]$x^T$ + {b}\")\n",
    "    return (w0, w1, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise (1m)\n",
    "\n",
    "Find the values for $\\mathbf{w}$ and $b$ that optimize the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "interact(linear_func,w0=(-10,10,1),w1=(-10,10,1),b=(-10,10,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution\n",
    "\n",
    "One solution is:\n",
    "\n",
    "$\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ and $b=-6$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def nonlinear_func(w0, w1, b):\n",
    "    w = array([[w0,w1]])\n",
    "    pred_func = lambda x: (w.dot(x.T) + b > 0).astype(int)\n",
    "    \n",
    "    plot_decision_boundary(pred_func, x=blob_df[[\"x0\", \"x1\"]].values, y=blob_df[\"y_non\"].values, points=1e5)\n",
    "    plt.title(f\"Accuracy for f(x) = [{w0}, {w1}]$x^T$ + {b}\")\n",
    "    return (w0, w1, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise (2m)\n",
    "\n",
    "Now, using this new dataset, find the values for $\\mathbf{w}$ and $b$ that optimize the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "interact(nonlinear_func,w0=(-20,20,1),w1=(-20,20,1),b=(-20,20,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution\n",
    "\n",
    "We top out at 75% with\n",
    "\n",
    "$\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ and $b=-6$\n",
    "\n",
    "\n",
    "Why? What would we need to do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Options: \n",
    "- We might be able to find a better solution if we could draw more lines.\n",
    "- Or possibly if we weren't limited to straight lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Logistic regression [[see lesson](../logistic-regression)] is an example of a linear classifier. Recall that the prediction function for logistic regression is \n",
    "\n",
    "$$f(\\mathbf{x}) = \\sigma(\\mathbf{w}^\\top\\mathbf{x}+b)$$\n",
    "\n",
    "The sigmoid function *is* nonlinear, however it only serves to convert the inner linear operation to a binary probability. We classify observations using some threshold, typically \n",
    "\n",
    "$$f(\\mathbf{x}) = \\begin{cases} \n",
    "    0 & \\sigma(\\mathbf{w}^\\top\\mathbf{x}+b) < .5 \\\\\n",
    "    1 & \\text{else}\n",
    "\\end{cases} $$\n",
    "\n",
    "Which is equivalent to \n",
    "\n",
    "$$f(\\mathbf{x}) = \\begin{cases} \n",
    "    0 & \\mathbf{w}^\\top\\mathbf{x}+b < 0 \\\\\n",
    "    1 & \\text{else}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Instead of digging into the math for *every* model we've covered, below you'll see a selection of some linear and nonlinear models and get a look at how they perform on different kinds of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, naive_bayes, svm, neural_network\n",
    "\n",
    "models = [(\"Logistic Regression\",\n",
    "           linear_model.LogisticRegression(solver='lbfgs')),\n",
    "          (\"NaÃ¯ve Bayes\", naive_bayes.GaussianNB()),\n",
    "          (\"Linear SVM\", svm.LinearSVC()),\n",
    "          (\"Linear MLP\", neural_network.MLPClassifier(activation=\"identity\"))]\n",
    "\n",
    "fig, axes = subplots(\n",
    "    nrows=2, ncols=len(models), squeeze=True, figsize=(4 * len(models), 8))\n",
    "\n",
    "for i, model_tup in enumerate(models):\n",
    "    name, model = model_tup\n",
    "\n",
    "    axes[0, i].set_title(name)\n",
    "    \n",
    "    # plot on seperable dataset\n",
    "    model.fit(X=blob_df[[\"x0\", \"x1\"]], y=blob_df[\"y_sep\"])\n",
    "    plot_decision_boundary(\n",
    "        model.predict,\n",
    "        x=blob_df[[\"x0\", \"x1\"]].values,\n",
    "        y=blob_df[\"y_sep\"].values,\n",
    "        ax=axes[0, i],\n",
    "        points=1e5)\n",
    "    \n",
    "    # plot on nonseperable dataset\n",
    "    model.fit(X=blob_df[[\"x0\", \"x1\"]], y=blob_df[\"y_non\"])\n",
    "    plot_decision_boundary(\n",
    "        model.predict,\n",
    "        x=blob_df[[\"x0\", \"x1\"]].values,\n",
    "        y=blob_df[\"y_non\"].values,\n",
    "        ax=axes[1, i],\n",
    "        points=1e5)\n",
    "    \n",
    "axes[0,0].set_ylabel(\"Separable Data\")\n",
    "axes[1,0].set_ylabel(\"Non-Separable Data\")\n",
    "fig.suptitle(\"Linear Model Performance on Separable and Non-separable Data\")\n",
    "savefig(\"images/lin_models.png\", dpi=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors, ensemble\n",
    "\n",
    "models = [(\"KNN\",\n",
    "           neighbors.KNeighborsClassifier()),\n",
    "          (\"Random Forest\", ensemble.RandomForestClassifier(n_estimators=100)),\n",
    "          (\"Radial SVM\", svm.SVC(gamma=\"scale\")),\n",
    "          (\"Relu MLP\", neural_network.MLPClassifier())]\n",
    "\n",
    "fig, axes = subplots(\n",
    "    nrows=2, ncols=len(models), squeeze=True, figsize=(4 * len(models), 8))\n",
    "\n",
    "for i, model_tup in enumerate(models):\n",
    "    name, model = model_tup\n",
    "\n",
    "    axes[0, i].set_title(name)\n",
    "    \n",
    "    # plot on seperable dataset\n",
    "    model.fit(X=blob_df[[\"x0\", \"x1\"]], y=blob_df[\"y_sep\"])\n",
    "    plot_decision_boundary(\n",
    "        model.predict,\n",
    "        x=blob_df[[\"x0\", \"x1\"]].values,\n",
    "        y=blob_df[\"y_sep\"].values,\n",
    "        ax=axes[0, i],\n",
    "        points=1e5)\n",
    "    \n",
    "    # plot on nonseperable dataset\n",
    "    model.fit(X=blob_df[[\"x0\", \"x1\"]], y=blob_df[\"y_non\"])\n",
    "    plot_decision_boundary(\n",
    "        model.predict,\n",
    "        x=blob_df[[\"x0\", \"x1\"]].values,\n",
    "        y=blob_df[\"y_non\"].values,\n",
    "        ax=axes[1, i],\n",
    "        points=1e5)\n",
    "    \n",
    "axes[0,0].set_ylabel(\"Separable Data\")\n",
    "axes[1,0].set_ylabel(\"Non-Separable Data\")\n",
    "fig.suptitle(\"Non-Linear Model Performance on Separable and Non-separable Data\")\n",
    "savefig(\"images/nonlin_models.png\", dpi=200);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What now? (class input)\n",
    "\n",
    "- Are linear models just worse?\n",
    "- How do we choose between these two classes of models: linear vs nonlinear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What now? (class input)\n",
    "\n",
    "- Are linear models just worse?\n",
    "    - Linear models are often more appropriate when:\n",
    "        - We need a model with a strong bias. \n",
    "        - We care more about interpretation than prediction.\n",
    "- How do we choose between these two classes of models: linear vs nonlinear?\n",
    "    - Carefully consider your\n",
    "        - Data: is it likely to require a nonlinear model or not?\n",
    "        - Priorities: What are you trying to accomplish?\n",
    "        - Project plan: It's often more appropriate to start with a simpler, linear model, and then move to a more complex model as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "\n",
    "- Linear functions are made up of two operations: matrix multiplication and addition.\n",
    "    - Models made up of only linear operations are unable to learn non-linear functions.\n",
    "\n",
    "- Linear models:\n",
    "    - Logistic Regression\n",
    "    - Neural Nets (without activation functions)\n",
    "    - Linear SVMs\n",
    "    - NaÃ¯ve Bayes (nonlinear but similarly limited) \n",
    "- Nonlinear\n",
    "    - KNN\n",
    "    - Random Forests\n",
    "    - SVMs with nonlinear kernels\n",
    "    - Neural Nets (with activation functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
