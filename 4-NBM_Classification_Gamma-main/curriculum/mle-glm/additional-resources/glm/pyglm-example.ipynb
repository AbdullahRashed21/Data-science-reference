{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Modeling with PyGLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/cscherrer/py-glm.git\n",
    "\n",
    "from glm.glm import GLM\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats('svg')\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some data for coal mining disasters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,\n",
    "     3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,\n",
    "     2, 2, 3, 4, 2, 1, 3, np.nan, 2, 1, 1, 1, 1, 3, 0, 0,\n",
    "     1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,\n",
    "     0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,\n",
    "     3, 3, 1, np.nan, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,\n",
    "     0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])\n",
    "\n",
    "x = np.arange(1851, 1962)\n",
    "\n",
    "mask = ~np.isnan(y)\n",
    "x = x[mask]\n",
    "y = y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y, '.');\n",
    "plt.ylabel(\"Disaster count\")\n",
    "plt.xlabel(\"Year\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the accident rate change over time? Let's use Poisson regression to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glm.families import Poisson\n",
    "poissonReg = GLM(family=Poisson())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones_like(y),x]\n",
    "poissonReg.fit(X,y)\n",
    "yhat = poissonReg.predict(X)\n",
    "residuals = poissonReg.deviance_residuals(X,y)\n",
    "\n",
    "plt.plot(x,y, '.');\n",
    "plt.plot(x,yhat)\n",
    "plt.ylabel(\"Disaster count\")\n",
    "plt.xlabel(\"Year\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyGLM gives us the $p$-values for free:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poissonReg.p_values_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we could also calculate this. Say we have a model $\\mathcal{M}_1$ with $p_1$ parameters and a submodel $\\mathcal{M}_0$ with $p_0$ parameters. Then under the null hypothesis ($H_0$: Data follow $M_0$), the difference in deviance is asymptotically (\"given enough data\") distributed as a $\\chi^2$ with $\\Delta p = p_1 - p_0$ degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = poissonReg.clone()\n",
    "m0.fit(np.ones_like(y).reshape(-1,1),y)\n",
    "m0.deviance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poissonReg.deviance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "1 - chi2(1).cdf(m0.deviance_ - poissonReg.deviance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite a bit different than the value we saw above, because PyGLM uses a [_Wald test_](https://en.wikipedia.org/wiki/Wald_test), while the result we described above uses [_Wilks' theorem_](https://en.wikipedia.org/wiki/Wilks%27_theorem). These are both asymptotic tests, and should give very similar results (or at least similar _conclusions_) in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the deviance residuals. The `myPlot` code below uses _generalized additive models_ to make it easier to see any trends in the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge scikit-sparse nose\n",
    "# !pip install pygam\n",
    "\n",
    "from pygam import ExpectileGAM\n",
    "\n",
    "def myPlot(m, x,y):\n",
    "    X = np.c_[np.ones_like(y),x]\n",
    "    gam50 = ExpectileGAM(expectile=0.5).gridsearch(X, y)\n",
    "\n",
    "    # and copy the smoothing to the other models\n",
    "    lam = gam50.lam\n",
    "\n",
    "    # now fit a few more models\n",
    "    gam95 = ExpectileGAM(expectile=0.95, lam=lam).fit(X, y)\n",
    "    gam75 = ExpectileGAM(expectile=0.75, lam=lam).fit(X, y)\n",
    "    gam25 = ExpectileGAM(expectile=0.25, lam=lam).fit(X, y)\n",
    "    gam05 = ExpectileGAM(expectile=0.05, lam=lam).fit(X, y)\n",
    "\n",
    "\n",
    "    plt.plot(x,y,'.')\n",
    "    plt.plot(x,gam50.predict(X))\n",
    "    plt.fill_between(x,gam05.predict(X),gam95.predict(X),color='C1',alpha=0.1)\n",
    "    plt.fill_between(x,gam25.predict(X),gam75.predict(X),color='C1',alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `myPlot` defined, we can have a look at the deviance residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPlot(poissonReg,x,residuals)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Deviance Residual\")\n",
    "plt.hlines(0,np.min(x),np.max(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's some strong variability, so we're probably missing a feature. Let's try adding a _change point_, a feature of the form `x > k`. This will allow for a \"break\" in the plot to account for some drmamatic change (maybe OSHA regulations?).\n",
    "\n",
    "But where should we put the change point? It's not obvious, but looks like in should be somewhere in the 1880-1940 range. Let's try the possibilities and see which gives us the best deviance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(1880,1940)\n",
    "\n",
    "# Pre-allocate an array to populate in the loop. \n",
    "# For larger arrays, this can help performance a lot, so it's a good habit :)\n",
    "deviances = np.zeros_like(years)\n",
    "\n",
    "for j,changept in enumerate(years):\n",
    "    newX = np.c_[np.ones_like(x),x, x>changept]\n",
    "    poissonReg.fit(newX,y)\n",
    "    deviances[j] = poissonReg.deviance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(years,deviances);\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Deviance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones_like(x),x, x>1891]\n",
    "\n",
    "poissonReg.fit(X,y)\n",
    "residuals=poissonReg.deviance_residuals(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did that help? Let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPlot(poissonReg,x,residuals)\n",
    "plt.hlines(0,np.min(x),np.max(x))\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Deviance Residual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poissonReg.fit(X,y)\n",
    "yhat = poissonReg.predict(X)\n",
    "residuals = poissonReg.deviance_residuals(X,y)\n",
    "\n",
    "plt.plot(x,y, '.');\n",
    "plt.plot(x,yhat)\n",
    "plt.ylabel(\"Disaster count\")\n",
    "plt.xlabel(\"Year\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Simulation`: Bootstrapping GLM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an earlier lecture, we saw bootstrapping as the first ingredient in bagging. More specifically, that was a _nonparametric_ bootstrap. PyGLM gives us a way to so this easily for a given model. First we build a `Simulation` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glm.simulation import Simulation\n",
    "sim = Simulation(poissonReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the general structure is\n",
    "\n",
    "```python\n",
    "for m in sim.non_parametric_bootstrap(X,y):\n",
    "    # Do something with `m`\n",
    "```\n",
    "\n",
    "Maybe the simplest thing we could do is to just plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y, '.');\n",
    "plt.plot(x,yhat)\n",
    "plt.ylabel(\"Disaster count\")\n",
    "plt.xlabel(\"Year\");\n",
    "for m in sim.non_parametric_bootstrap(X,y):\n",
    "    plt.plot(x,m.predict(X),c='C1',alpha=0.02,lw=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bootstrap tells us what kind of behavior we can expect on data simulated in the same way. We can think of the results as \"possible worlds\". This gives us some insight into variability and the plausibility of simpler models.\n",
    "\n",
    "For example, look at the years before the changepoint. The bootstrap results tell us that this segment suffers from much higher variance than that after the changepoint. Maybe we can constrain things to improve this. \n",
    "\n",
    "Luckily, there's an obvious path; a \"constant on each segment\" model is well within range. This is supported by the $p$-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poissonReg.p_values_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That 0.353 is way out of line; let's get rid of it and check again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones_like(x), x>1890]\n",
    "\n",
    "poissonReg.fit(X,y)\n",
    "yhat = poissonReg.predict(X)\n",
    "\n",
    "residuals=poissonReg.deviance_residuals(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y, '.');\n",
    "plt.plot(x,yhat)\n",
    "plt.ylabel(\"Disaster count\")\n",
    "plt.xlabel(\"Year\");\n",
    "for m in sim.non_parametric_bootstrap(X,y):\n",
    "    plt.plot(x,m.predict(X),c='C1',alpha=0.01,lw=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just to make sure we didn't screw up the residuals..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPlot(poissonReg,x,residuals)\n",
    "plt.hlines(0,np.min(x),np.max(x))\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Deviance Residual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Consderations\n",
    "\n",
    "A $\\text{Poisson}(\\lambda)$ distribution has mean $\\lambda$ and also variance $\\lambda$. Sometimes we need higher variance (due to additional sources of noise, etc). This can be done with a _dispersion_ parameter included in the `QuasiPoisson` family (`Poisson` constrains dispersion to equal one). It's also common to use a `NegativeBinomial` for this, but the effect can be very different.\n",
    "\n",
    "Counts usually count events in some time period, but the \"exposure\" durations may be be uniform. This can be expressed by adding an `offset` keyword argument to the `.fit` method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
